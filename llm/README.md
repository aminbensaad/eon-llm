### README.md - LLM 

# Project Setup üõ†Ô∏è

![Setup](https://drive.google.com/uc?id=12fwXHFsfRBz2R1S9WgwP8VADEhVQrTQm)


### Step 1: Create the Conda Environment 'eon-llm'
First, ensure you have Conda installed (and 'Libmamba' enabled as resolver - much faster). Then run the following commands:

```sh
conda env create -f environment.yml
```

### Step 2: Activate the Environment
Activate the newly created environment:

```sh
conda activate eon-llm
```

## Step 3: Install (Potentially Missing) Packages with pip
With the `eon-llm` environment activated, install the required packages using the `requirements.txt` file provided:

```sh
pip install -r requirements.txt
```
cd
### Step 4: Test if Packages Have Been Installed Properly
Run model predictions (`-p`) or (re-)run evaluation metrics (`-e`) for testing, for instance:

```sh
python scripts/run.py tuned -p
python run.py -d SQuAD G -m tuned Gtuned -p -e --all
```

# Project Structure üóÇÔ∏è

This directory covers all LLM-related research directories and files, including:

**data:**
- The data (GermanQuAD and SQuAD) as provided by E.ON and used for fine-tuning (`train.json`) and prediction (`test/dev.json`).

**eval_results:**
- The summarized evaluation results for the defined metrics (e.g., BLEU, ROUGE, F1, exact match, ...) in 'metrics'.

**figures:**
- The resulting figures from our plotting of evaluation results in 'experiments.ipynb'.

**metrics:**
- The defined metrics to evaluate model results.

**model_results:**
- The predictions as generated by the different tested models.

**models:**
- Contains the scripts to run inference on different models

**local_models:**
- Local models which cannot be found on Hugging Face, for example manually fine-tuned models.
  Models can be added in Hugging Face model format as `local_models/<model_name>/<checkpoint>`
  with `"<model_name>/<checkpoint>"` added as a new model to the "local" key in `scripts/model_ids.py`.

**scripts:**
- The scripts to execute model fine-tuning (`fine-tune.py`), run predictions (`run.py` with commands `base.py` for pre-trained models and `tuned.py` for fine-tuned models), and utility functions (`utils.py`).

**experiments.ipynb:**
- This Jupyter notebook is used for experimentation and to generate visualizations for model & evaluation results.

**timing_results.json:**
- This file contains the results of the utils function 'store_timing_results()' which captures the inference times for each model on SQuAD v2 and GermanQuAD


--------------------

**IMPORTANT:**
- Results (evaluation or models) are always separated by model types (i.e., base, Gbase, tuned, Gtuned).

Please update the content in case of changes. 
