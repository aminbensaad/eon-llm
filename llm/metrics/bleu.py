"""
BLEU Evaluation Script

This script computes the BLEU (Bilingual Evaluation Understudy) score for model predictions
against a reference dataset. The BLEU score is a metric for evaluating a generated sentence
to a reference sentence, commonly used in machine translation and text generation tasks.

The script reads model predictions and the reference dataset, computes the BLEU score for each 
question-answer pair, and then averages these scores to produce an overall BLEU score for the model.

Usage:
    python bleu.py <predictions_path> <dataset_path> <output_path>
    
    predictions_path: Path to the JSON file containing the model predictions.
    dataset_path: Path to the JSON file containing the reference dataset (SQuAD format).
    output_path: Path to the JSON file where the BLEU score result will be saved.

Dependencies:
    nltk (for BLEU score computation)
"""

import sys

# subprocess.check_call([sys.executable, "-m", "pip", "install", "nltk"])

import json

# Ensure required packages are installed
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction


def compute_bleu(reference, hypothesis):
    """
    Computes the BLEU score for a single reference and hypothesis pair.

    Args:
        reference (str): The reference text.
        hypothesis (str): The hypothesis text generated by the model.

    Returns:
        float: The BLEU score.
    """
    smoothing_function = SmoothingFunction().method1
    return sentence_bleu([reference], hypothesis, smoothing_function=smoothing_function)


def main(predictions_path, dataset_path, output_path):
    """
    Main function to read the predictions and dataset, compute the BLEU score for each pair,
    and save the average BLEU score to the output file.

    Args:
        predictions_path (str): Path to the JSON file containing the model predictions.
        dataset_path (str): Path to the JSON file containing the reference dataset.
        output_path (str): Path to the JSON file where the BLEU score result will be saved.
    """
    # Load model predictions
    with open(predictions_path, "r") as f:
        predictions = json.load(f)

    # Load reference dataset
    with open(dataset_path, "r") as f:
        dataset = json.load(f)

    # Initialize scores
    overall_scores = []
    has_ans_scores = []
    no_ans_scores = []

    # Compute BLEU scores for each question-answer pair
    for article in dataset["data"]:
        for paragraph in article["paragraphs"]:
            for qa in paragraph["qas"]:
                qid = str(qa["id"])
                reference = qa["answers"][0]["text"] if qa["answers"] else ""
                hypothesis = predictions.get(qid, "")
                score = compute_bleu(reference, hypothesis)
                overall_scores.append(score)

                if qa["is_impossible"]:
                    no_ans_scores.append(score)
                else:
                    has_ans_scores.append(score)

    # Compute average BLEU scores
    avg_bleu = sum(overall_scores) / len(overall_scores) if overall_scores else 0.0
    has_ans_bleu = sum(has_ans_scores) / len(has_ans_scores) if has_ans_scores else 0.0
    no_ans_bleu = sum(no_ans_scores) / len(no_ans_scores) if no_ans_scores else 0.0

    # Save the average BLEU scores to the output file
    with open(output_path, "w") as f:
        json.dump(
            {"bleu": avg_bleu, "HasAns_bleu": has_ans_bleu, "NoAns_bleu": no_ans_bleu},
            f,
            indent=2,
        )


if __name__ == "__main__":
    # Ensure the correct number of arguments are provided
    if len(sys.argv) != 4:
        print("Usage: python bleu.py <predictions_path> <dataset_path> <output_path>")
        sys.exit(1)

    # Parse command line arguments
    predictions_path = sys.argv[1]
    dataset_path = sys.argv[2]
    output_path = sys.argv[3]

    # Run the main function
    main(predictions_path, dataset_path, output_path)
