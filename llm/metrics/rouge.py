"""
ROUGE Evaluation Script

This script computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores for
model predictions against a reference dataset. ROUGE is a set of metrics for evaluating automatic
summarization and machine translation, particularly focusing on recall.

The script reads model predictions and the reference dataset, computes the ROUGE scores for each 
question-answer pair, and then averages these scores to produce overall ROUGE-1, ROUGE-2, and ROUGE-L 
scores for the model.

Usage:
    python rouge_metric.py <predictions_path> <dataset_path> <output_path>
    
    predictions_path: Path to the JSON file containing the model predictions.
    dataset_path: Path to the JSON file containing the reference dataset (SQuAD format).
    output_path: Path to the JSON file where the ROUGE score results will be saved.

Dependencies:
    rouge-score (for ROUGE score computation)
"""

import json
import sys

# Ensure required packages are installed
# subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip"])
# subprocess.check_call([sys.executable, "-m", "pip", "install", "rouge-score"])

from rouge_score import rouge_scorer


def compute_rouge(reference, hypothesis):
    """
    Computes the ROUGE scores for a single reference and hypothesis pair.

    Args:
        reference (str): The reference text.
        hypothesis (str): The hypothesis text generated by the model.

    Returns:
        dict: The ROUGE scores.
    """
    scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)
    return scores


def main(predictions_path, dataset_path, output_path):
    """
    Main function to read the predictions and dataset, compute the ROUGE scores for each pair,
    and save the average ROUGE scores to the output file.

    Args:
        predictions_path (str): Path to the JSON file containing the model predictions.
        dataset_path (str): Path to the JSON file containing the reference dataset.
        output_path (str): Path to the JSON file where the ROUGE score results will be saved.
    """
    # Load model predictions
    with open(predictions_path, "r") as f:
        predictions = json.load(f)

    # Load reference dataset
    with open(dataset_path, "r") as f:
        dataset = json.load(f)

    # Initialize scores
    all_scores = []
    has_ans_scores = []
    no_ans_scores = []

    # Compute ROUGE scores for each question-answer pair
    for article in dataset["data"]:
        for paragraph in article["paragraphs"]:
            for qa in paragraph["qas"]:
                qid = str(qa["id"])
                reference = qa["answers"][0]["text"] if qa["answers"] else ""
                hypothesis = predictions.get(qid, "")
                scores = compute_rouge(reference, hypothesis)
                all_scores.append(scores)

                if qa["is_impossible"]:
                    no_ans_scores.append(scores)
                else:
                    has_ans_scores.append(scores)

    # Function to average ROUGE scores
    def average_rouge_scores(scores_list):
        avg_rouge = {
            "rouge1": {"f": 0, "p": 0, "r": 0},
            "rouge2": {"f": 0, "p": 0, "r": 0},
            "rougeL": {"f": 0, "p": 0, "r": 0},
        }
        for scores in scores_list:
            for key in scores:
                avg_rouge[key]["f"] += scores[key].fmeasure
                avg_rouge[key]["p"] += scores[key].precision
                avg_rouge[key]["r"] += scores[key].recall

        for key in avg_rouge:
            for sub_key in avg_rouge[key]:
                avg_rouge[key][sub_key] /= len(scores_list) if scores_list else 1

        return avg_rouge

    # Compute average ROUGE scores
    avg_rouge = average_rouge_scores(all_scores)
    has_ans_rouge = average_rouge_scores(has_ans_scores)
    no_ans_rouge = average_rouge_scores(no_ans_scores)

    # Save the average ROUGE scores to the output file
    with open(output_path, "w") as f:
        json.dump(
            {
                "rouge": avg_rouge,
                "HasAns_rouge": has_ans_rouge,
                "NoAns_rouge": no_ans_rouge,
            },
            f,
            indent=2,
        )


if __name__ == "__main__":
    # Ensure the correct number of arguments are provided
    if len(sys.argv) != 4:
        print(
            "Usage: python rouge_metric.py <predictions_path> <dataset_path> <output_path>"
        )
        sys.exit(1)

    # Parse command line arguments
    predictions_path = sys.argv[1]
    dataset_path = sys.argv[2]
    output_path = sys.argv[3]

    # Run the main function
    main(predictions_path, dataset_path, output_path)
